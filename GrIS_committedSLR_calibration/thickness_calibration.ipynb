{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "imperial-collect",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-nightmare",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import warnings\n",
    "\n",
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "\n",
    "sys.path.append('../utilities')\n",
    "import GrIS_committedSLR_calibration_utilities as utils\n",
    "\n",
    "import boto3\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-blend",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-meeting",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_year = 2007.\n",
    "end_year = 2015.\n",
    "\n",
    "grid_size = 50000.\n",
    "\n",
    "grid_extent = np.array([-720500.0, 960500.0, -3450500.0, -569500.0])\n",
    "\n",
    "dh_mod_obs_sigma_multiplier = 50.\n",
    "dhThresholdMax = np.nan #-5\n",
    "\n",
    "debug_plots = True\n",
    "\n",
    "start_year_string = '{:4d}'.format(int(start_year))[2:]\n",
    "end_year_string   = '{:4d}'.format(int(end_year)  )[2:]\n",
    "\n",
    "# For saving the results ...\n",
    "run_name = 'thickness_sigmamodx{:.0f}'.format(dh_mod_obs_sigma_multiplier)\n",
    "run_description = 'start_year = {:4.0f}; end_year = {:4.0f}; '.format(start_year, end_year) + \\\n",
    "                  'grid_size = {:8.0f}; dh_mod_obs_sigma_multiplier = {:5.0f}; dhThresholdMax = {:f}'.format(grid_size, dh_mod_obs_sigma_multiplier, dhThresholdMax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-concern",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load total MAF / GMSL\n",
    "maf_cmmtt = dict()\n",
    "maf_ctrl = dict()\n",
    "gmsl_change_2100 = dict()\n",
    "gmsl_anomaly_change_2100 = dict()\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "maf_files = s3.glob('dh-gapc/GrIS_committed_SLR/netcdfs/MAF/*cmmtt*MAF.nc')\n",
    "\n",
    "ensembleIDs = list()\n",
    "for iFile, maf_file in enumerate(maf_files):\n",
    "    ensembleID = os.path.basename(maf_file).split('.')[3]\n",
    "    ensembleIDs.append(ensembleID)\n",
    "    \n",
    "    # Read cmmtt MAF netcdf\n",
    "    with s3.open(maf_file, 'rb') as f:\n",
    "        maf_ds = xr.open_dataset(f, engine='h5netcdf')\n",
    "\n",
    "        limnsw = maf_ds['limnsw'].data\n",
    "        maf_cmmtt[ensembleID] = limnsw\n",
    "    \n",
    "    \n",
    "    # Read ctrl MAF netcdf\n",
    "    maf_file = maf_file.replace('cmmtt','ctrl')\n",
    "    \n",
    "    with s3.open(maf_file, 'rb') as f:\n",
    "        maf_ds = xr.open_dataset(f, engine='h5netcdf')\n",
    "\n",
    "        limnsw = maf_ds['limnsw'].data\n",
    "        maf_ctrl[ensembleID] = limnsw\n",
    "    \n",
    "    \n",
    "    maf_anomaly = maf_cmmtt[ensembleID] - maf_ctrl[ensembleID]\n",
    "    maf_anomaly_change_2100 = maf_anomaly[-1] - maf_anomaly[1]\n",
    "    gmsl_anomaly_change_2100[ensembleID] = -(maf_anomaly_change_2100/1e12)/361.8\n",
    "    \n",
    "    maf_change_2100 = maf_cmmtt[ensembleID][-1] - maf_cmmtt[ensembleID][0]\n",
    "    gmsl_change_2100[ensembleID] = -(maf_change_2100/1e12)/361.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-occupation",
   "metadata": {},
   "source": [
    "# Read SERAC observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-restriction",
   "metadata": {},
   "source": [
    "<h3><font color='red'>NOTE:</font></h3>\n",
    "\n",
    "Bea suggested using DPS (ALPS) when abs(dh) > 5m and DTS (poly fit) when abs(dh) < 5m because ALPS does a good job at fitting dh in highly dynamic areas and the poly fit does a better job in the places where there's little dynamic dh (because ALPS can actually overfit to errors in these places. Can make this mod in utils.read_SERAC_obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-collins",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read dynamic dh from SERAC\n",
    "dh_dyn_obs = utils.read_SERAC_obs('SERAC/SERACAnnualRates2021JanIGSPres.csv')\n",
    "\n",
    "# select obs\n",
    "dh_dyn_obs_selected = utils.select_dh_obs(dh_dyn_obs, startYear=start_year, endYear=end_year, dhThresholdMax=dhThresholdMax)\n",
    "print('selected number of obs: {:8d}'.format(len(dh_dyn_obs_selected)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-stranger",
   "metadata": {},
   "source": [
    "## Grid the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-cherry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup grid\n",
    "x_centers = np.arange(grid_extent[0]+grid_size/2, grid_extent[1]-grid_size/2, grid_size)\n",
    "y_centers = np.arange(grid_extent[2]+grid_size/2, grid_extent[3]-grid_size/2, grid_size)\n",
    "\n",
    "# Setup another grid at 50 km for plotting purposes\n",
    "grid_size_plot = 50000.\n",
    "x_centers_plot = np.arange(grid_extent[0]+grid_size_plot/2, grid_extent[1]-grid_size_plot/2, grid_size_plot)\n",
    "y_centers_plot = np.arange(grid_extent[2]+grid_size_plot/2, grid_extent[3]-grid_size_plot/2, grid_size_plot)\n",
    "\n",
    "\n",
    "# Grid the observations\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "    dh_obs_grid, dh_obs_sigma_grid = utils.grid_obs_dh(x_centers, y_centers, grid_size, grid_size, dh_dyn_obs_selected, \\\n",
    "                                                                                         startYear=start_year, endYear=end_year)\n",
    "    dh_obs_grid_plot, dh_obs_sigma_grid_plot = utils.grid_obs_dh( \\\n",
    "                                    x_centers_plot, y_centers_plot, grid_size_plot, grid_size_plot, \\\n",
    "                                    dh_dyn_obs_selected, startYear=start_year, endYear=end_year)\n",
    "    \n",
    "# Plot\n",
    "if debug_plots:\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    \n",
    "    im_dh = ax.scatter([d['x']/1000 for d in dh_dyn_obs_selected], \\\n",
    "                       [d['y']/1000 for d in dh_dyn_obs_selected], \\\n",
    "                       c=[d['dh' + start_year_string + end_year_string] for d in dh_dyn_obs_selected], \\\n",
    "                       s=5., cmap='RdBu', vmin=-20, vmax=20)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "fig, ax = plt.subplots(1,2,figsize=(5,4),dpi=300)\n",
    "im_dh = ax[0].imshow(dh_obs_grid_plot, extent=[e/1000. for e in grid_extent], \\\n",
    "           cmap='RdBu', vmin=-20, vmax=20, origin='lower', \\\n",
    "           aspect='auto')\n",
    "\n",
    "im_er = ax[1].imshow(dh_obs_sigma_grid_plot, extent=[e/1000. for e in grid_extent], \\\n",
    "           cmap='Reds', vmin=0, vmax=2, origin='lower', \\\n",
    "           aspect='auto')\n",
    "\n",
    "cb = fig.colorbar(im_dh, orientation='horizontal', ax=ax[0], pad=0.06, shrink=0.5)\n",
    "cb.set_label('dh (m)')\n",
    "cb = fig.colorbar(im_er, orientation='horizontal', ax=ax[1], pad=0.06, shrink=0.5)\n",
    "cb.set_label('$\\sigma$ (m)')\n",
    "\n",
    "ax[0].set_title('gridded obs')\n",
    "ax[1].set_title('obs error')\n",
    "\n",
    "x_lim = ax[0].get_xlim()\n",
    "y_lim = ax[0].get_ylim()\n",
    "ax[0].set_xlim(x_lim)\n",
    "ax[0].set_ylim(y_lim)\n",
    "ax[1].set_xlim(x_lim)\n",
    "ax[1].set_ylim(y_lim)\n",
    "\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "\n",
    "#fig.savefig('plots/SERAC_obs.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-receptor",
   "metadata": {},
   "source": [
    "# Calculate residuals\n",
    "\n",
    "<h3><font color='red'>NOTE:</font></h3> This can take a few minutes to run ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all model files\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "names = s3.glob('dh-gapc/GrIS_committed_SLR/zarr/dhdynAnom/*6079.nc.zarr')\n",
    "#names = s3.glob('dh-gapc/GrIS_committed_SLR/zarr/dhdynAnom_old/*6079.nc.zarr')\n",
    "#names = s3.glob('dh-gapc/GrIS_committed_SLR/zarr/dhdynAnom_new/*.nc.zarr')\n",
    "\n",
    "# DEBUG\n",
    "#names = names[:4]\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "if False:\n",
    "    for name in names:\n",
    "        print(name)\n",
    "        s_j, mod_obs_diff_grid = utils.grid_models_and_calculate_residuals(name, x_centers, y_centers, grid_size, \\\n",
    "                                  dh_obs_grid, dh_obs_sigma_grid, dh_mod_obs_sigma_multiplier)\n",
    "        break\n",
    "\n",
    "if True:\n",
    "    # Loop through model files and return statistics and scores\n",
    "    with tqdm_joblib(tqdm(desc='Looping through models', total=len(names))) as progress_bar:\n",
    "        results = Parallel(n_jobs=4)(delayed(utils.grid_models_and_calculate_residuals) \\\n",
    "                                 (name, x_centers, y_centers, grid_size, \\\n",
    "                                  dh_obs_grid, dh_obs_sigma_grid, dh_mod_obs_sigma_multiplier) for name in names)\n",
    "\n",
    "    mod_obs_diff_grids = [r[1] for r in results]\n",
    "    s_j = np.array([r[0] for r in results])\n",
    "    w_j = s_j / np.sum(s_j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-narrative",
   "metadata": {},
   "source": [
    "### Intermediate checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,1.0,0.02)\n",
    "\n",
    "w_j_scaled = w_j / np.max(w_j)\n",
    "\n",
    "# Plot the values of the weights\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "ax[0].plot(w_j_scaled, 'o', label='SERAC weights')\n",
    "ax[0].set_ylim(-0.05, 1.20)\n",
    "ax[0].legend()\n",
    "ax[1].hist(w_j_scaled, bins=bins, alpha=0.5, label='SERAC weights')\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-scope",
   "metadata": {},
   "source": [
    "## Plot highest/lowest weighted ensemble members from SERAC calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-shift",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the observations with the ensemble members with the highest and lowest weights\n",
    "w_j_sorted_idx = sorted(range(len(w_j)), key=lambda i: w_j[i])\n",
    "name_highest_weight = names[w_j_sorted_idx[-1]]\n",
    "name_lowest_weight  = names[w_j_sorted_idx[ 0]]\n",
    "\n",
    "\n",
    "for mod_name in (name_highest_weight, name_lowest_weight):\n",
    "    dh_mod_grid = utils.grid_mod_dh \\\n",
    "                                (mod_name, x_centers, y_centers, grid_size, grid_size) \\\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1,4,figsize=(16,8))\n",
    "\n",
    "    im_dh = ax[0].imshow(dh_obs_grid, extent=[e/1000. for e in grid_extent], \\\n",
    "                   cmap='RdBu', vmin=-20, vmax=20, origin='lower', \\\n",
    "                   aspect='auto')\n",
    "\n",
    "    im_dh = ax[1].imshow(dh_mod_grid, extent=[e/1000. for e in grid_extent], \\\n",
    "                   cmap='RdBu', vmin=-20, vmax=20, origin='lower', \\\n",
    "                   aspect='auto')\n",
    "\n",
    "    residuals = dh_mod_grid - dh_obs_grid\n",
    "    im_dh = ax[2].imshow(residuals, extent=[e/1000. for e in grid_extent], \\\n",
    "                   cmap='RdBu', vmin=-20, vmax=20, origin='lower', \\\n",
    "                   aspect='auto')\n",
    "\n",
    "    cb = plt.colorbar(im_dh, orientation='horizontal', ax=ax[0], pad=0.06)\n",
    "    cb.set_label('dh (m)')\n",
    "    cb = plt.colorbar(im_dh, orientation='horizontal', ax=ax[1], pad=0.06)\n",
    "    cb.set_label('dh (m)')\n",
    "    cb = plt.colorbar(im_dh, orientation='horizontal', ax=ax[2], pad=0.06)\n",
    "    cb.set_label('dh (m)')\n",
    "    \n",
    "    residuals = residuals.flatten()\n",
    "    residuals = residuals[~np.isnan(residuals)]\n",
    "    ax[3].hist(residuals, bins=50)\n",
    "    \n",
    "    fig.suptitle(mod_name)\n",
    "    ax[0].set_title('gridded obs')\n",
    "    ax[1].set_title('gridded mod')\n",
    "    ax[2].set_title('residuals')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-sense",
   "metadata": {},
   "source": [
    "# Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-security",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "if os.path.isdir('results/' + run_name):\n",
    "    print('WARNING: Results directory with name ' + run_name + ' already exists!')\n",
    "\n",
    "else:\n",
    "    print('Saving ' + run_name)\n",
    "    os.mkdir('results/' + run_name)\n",
    "\n",
    "    txt_file = open('results/' + run_name + '/description.txt', 'w')\n",
    "    txt_file.write(run_description)\n",
    "    txt_file.close()\n",
    "\n",
    "    pickle.dump({'r': mod_obs_diff_grids, 's_j': s_j, 'w_j': w_j}, open('results/' + run_name + '/weights.p', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-orientation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
